{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load sequences, labels, protein lengths for train/valid/test\n",
    "train = torch.load('data/yst_train.pt')\n",
    "valid = torch.load('data/yst_valid.pt')\n",
    "test = torch.load('data/yst_test.pt')\n",
    "\n",
    "train_lengths = torch.from_numpy(np.load('data/yst_train_lengths.npy'))\n",
    "valid_lengths = torch.from_numpy(np.load('data/yst_valid_lengths.npy'))\n",
    "test_lengths = torch.from_numpy(np.load('data/yst_test_lengths.npy'))\n",
    "\n",
    "y_train = torch.from_numpy(np.load('data/yst_train_labels.npy'))\n",
    "y_valid = torch.from_numpy(np.load('data/yst_valid_labels.npy'))\n",
    "y_test = torch.from_numpy(np.load('data/yst_test_labels.npy'))\n",
    "\n",
    "yst_go_terms = np.load('data/yst_go_terms.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"\n",
    "    FastText model\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "       \n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        # TODO: replace with your code\n",
    "        self.embed = nn.Embedding(vocab_size+1, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,num_labels)\n",
    "        \n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        # TODO: replace with your code\n",
    "        out = torch.sum(self.embed(data.long()),dim=1)\n",
    "        length = length.float().view((-1,1))\n",
    "        out = self.linear(out/length)\n",
    "        \n",
    "        return nn.functional.sigmoid(out)\n",
    "    \n",
    "def train_test(tr_seq,tr_len,tr_lab,tst_seq,tst_len,tst_lab):\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    train_sequences = Variable(tr_seq)\n",
    "    train_lens = Variable(tr_len)\n",
    "    train_labels = Variable(tr_lab)\n",
    "    valid_sequences = Variable(tst_seq)\n",
    "    valid_lens = Variable(tst_len)\n",
    "    valid_label = tst_lab.numpy()\n",
    "    \n",
    "    for j in range(data_size//batch_size):\n",
    "        batch = j*batch_size\n",
    "        sequence = train_sequences[batch:batch+batch_size]\n",
    "        length = train_lens[batch:batch+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequence, length)\n",
    "        loss = criterion(outputs.float(),train_labels[batch:batch+batch_size].view((-1,1)).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_outputs = model(valid_sequences,valid_lens)\n",
    "    model.train()\n",
    "    \n",
    "    predicted = np.round(val_outputs.data.numpy())\n",
    "    total_predictions = valid_label.size\n",
    "    accuracy = np.sum(predicted==valid_label)/total_predictions\n",
    "    \n",
    "    print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (some of the) hyper parameters\n",
    "learning_rate = 0.001\n",
    "vocab_size = 26 # number words in the vocabulary base\n",
    "emb_dim = 50 # dimension for n-gram embedding\n",
    "num_epochs = 1 # number epoch to train\n",
    "batch_size = 26\n",
    "data_size = train.shape[0]\n",
    "num_labels = len(yst_go_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FastText(vocab_size, emb_dim)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielamaranto/anaconda/lib/python3.6/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([676, 1])) that is different to the input size (torch.Size([26, 26])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.864965252816\n"
     ]
    }
   ],
   "source": [
    "train_test(train,train_lengths,y_train,valid,valid_lengths,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
